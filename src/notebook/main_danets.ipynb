{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Librart Imports\n",
    "##################\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "from data.dataset import CustomDataset, CustomDataModule\n",
    "from model.danets_baseline import CustomDANETs\n",
    "from model.lit import LightningModel\n",
    "\n",
    "from utils.config_loader import load_config\n",
    "CFG = load_config('../config/experiment/danets_baseline.yaml')\n",
    "\n",
    "def seed_everything(seed=CFG['seed']):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark =True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Cross validation with oof predicition results\n",
    "\n",
    "following code run cross validation using fold number pre-defined using training data. \n",
    "CustomDataModule will automatically split train and validation dataset to start training process.\n",
    "Early stopping is enabled with custom loss score.\n",
    "\n",
    "DANETs with custom loss function has advantage over Gradient Boosting models such as LGBM, XGBoost as loss function customization is limited, and DANETs out-performed as shown in the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current local date and time\n",
    "current_date_time = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "formatted_date_time = current_date_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "train_df = \"YOUR_TRAIN_DATA\"\n",
    "train_df[\"oof_pred\"] = 0.0\n",
    "for fold in tqdm(range(CFG['n_folds']), desc=\"Folds\", total=CFG['n_folds']):\n",
    "\n",
    "    data_module = CustomDataModule(train_df, fold, CFG['meta_feats'])\n",
    "\n",
    "    pytorch_model = CustomDANETs(CFG['meta_feats'])\n",
    "    lightning_model = LightningModel(model=pytorch_model)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(dirpath=\"path/checkpoints/\", \n",
    "                        filename=f\"{CFG['model']}_fold{fold}\" + \"-{epoch:02d}-{valid_score:.5f}\",\n",
    "                        save_top_k=1, mode='min', monitor=\"valid_score\", verbose=True),\n",
    "        EarlyStopping(monitor='valid_score', patience=CFG['patience'], mode='max'),\n",
    "        LearningRateMonitor(logging_interval='epoch'),\n",
    "        StochasticWeightAveraging(swa_lrs=1e-3, swa_epoch_start=0.5, annealing_epochs=5, annealing_strategy='linear', \n",
    "                                  avg_fn=lambda averaged_model_parameter, model_parameter, num_averaged: \\\n",
    "                                    0.3 * averaged_model_parameter + 0.7 * model_parameter)\n",
    "        ]\n",
    "    csvlogger = CSVLogger(save_dir=\"logs/\", name=f\"{CFG['model']}_{formatted_date_time}_fold{fold}\", version=0)\n",
    "    tblogger = TensorBoardLogger(save_dir=\"logs/\", name=f\"{CFG['model']}_{formatted_date_time}_fold{fold}\", version=0)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CFG['max_epochs'],\n",
    "        callbacks=callbacks,\n",
    "        precision=\"16-mixed\",\n",
    "        accelerator=\"auto\",  # Uses GPUs or TPUs if available\n",
    "        devices=1,  # Uses all available GPUs/TPUs if applicable\n",
    "        logger=[csvlogger, tblogger],\n",
    "        log_every_n_steps=100,\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.fit(model=lightning_model, datamodule=data_module)\n",
    "\n",
    "    runtime = (time.time() - start_time) / 60\n",
    "    print(f\"Training took {runtime:.2f} min in total.\")\n",
    "\n",
    "    lightning_model.load_state_dict(torch.load(trainer.checkpoint_callback.best_model_path)['state_dict'])\n",
    "    preds = trainer.predict(lightning_model, data_module.val_dataloader())\n",
    "    train_df.loc[train_df['fold'] == fold, 'oof_pred'] = np.concatenate(preds, axis=0)\n",
    "\n",
    "    del trainer, lightning_model, pytorch_model, data_module; gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Experiment Tracking\n",
    "\n",
    "each fold result metric and loss behaivor is logged and visualized by executing next command in VS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%tensorboard --logdir logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "takehiro-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
